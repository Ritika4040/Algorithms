Divide and Conquer. This is a strategy where we break a big problem into smaller, similar subproblems, solve them, and then combine the results. 
It's recursive in nature and works great for problems that exhibit optimal substructure.

First, let's understand the three steps: Divide, Conquer, and Combine.

Divide: Split the problem into smaller subproblems.
Conquer: Solve each subproblem recursively. If the subproblem is small enough, solve it directly.
Combine: Merge the solutions to get the answer for the original problem.
[Visual: Shows a tree diagram where the root is the main problem, branching into subproblems.]

A classic example is Binary Search. Imagine you have a sorted array and need to find a target element. 
Instead of checking each element linearly, which is O(n), we use Divide and Conquer for O(log n).

Here's how: Compare the target with the middle element. If it matches, done. If the target is smaller, 
search the left half; if larger, the right half. Recursively repeat.

binarySearch(arr, low, high, target) {
    if (low > high) return -1;
    mid = (low + high) / 2;
    if (arr[mid] == target) return mid;
    else if (arr[mid] > target) return binarySearch(arr, low, mid-1, target);
    else return binarySearch(arr, mid+1, high, target);
}
Example: Array [1, 3, 5, 7, 9], target 7.

Mid = 5 (index 2), 5 < 7, so right half: [7, 9].
Mid = 7, found at index 3.
This halves the search space each time. Perfect for sorted data.

Now, let's move to sorting. Merge Sort is a Divide and Conquer sorting algorithm. It's stable, meaning it preserves the order of equal elements, and runs in O(n log n) time.

Steps:

Divide: Split the array into two halves.
Conquer: Recursively sort each half.
Combine: Merge the two sorted halves.
[Animation: Array splits like a tree, then merges back.]

mergeSort(arr, low, high) {
    if (low < high) {
        mid = (low + high) / 2;
        mergeSort(arr, low, mid);
        mergeSort(arr, mid+1, high);
        merge(arr, low, mid, high);
    }
}

merge(arr, low, mid, high) {
    // Create temp arrays, merge while comparing
}
Example: [8, 3, 1, 7].

Split: [8, 3] and [1, 7].
Sort: [3, 8] and [1, 7].
Merge: Compare 3 and 1 → take 1, then 3 and 7 → 3, 7, 8. Result: [1, 3, 7, 8].
Why O(n log n)? Each level merges O(n) elements, and there are log n levels.

Quick Sort is another one, invented by Tony Hoare. It's in-place, meaning no extra space like Merge Sort.

Steps:

Choose a pivot (often the last element).
Partition: Rearrange so elements less than pivot are left, greater are right.
Recursively sort left and right.

quickSort(arr, low, high) {
    if (low < high) {
        pi = partition(arr, low, high);
        quickSort(arr, low, pi-1);
        quickSort(arr, pi+1, high);
    }
}

partition(arr, low, high) {
    pivot = arr[high];
    i = low - 1;
    for (j = low; j < high; j++) {
        if (arr[j] <= pivot) {
            i++;
            swap(arr[i], arr[j]);
        }
    }
    swap(arr[i+1], arr[high]);
    return i+1;
}
Example: [10, 7, 8, 9, 1, 5], pivot 5.

Partition: [1, 5] left, [7, 8, 9, 10] right.
Recurse.
Average case: O(n log n). Worst case: O(n²) if pivot is always smallest/largest, like a sorted array. To fix, randomize pivot or use median-of-three.

[Diagram: Recursion tree for Quick Sort, showing depth.]

Other examples: Strassen's Matrix Multiplication reduces from O(n^3) to O(n^{2.81}) by dividing matrices into quadrants and using clever formulas.

Tower of Hanoi: Move n disks from A to C using B as auxiliary. Divide: Move n-1 to B, move bottom to C, move n-1 from B to C.

It's exponential time, but a fun recursive puzzle.

Closest Pair of Points: Divide plane into halves, find min in each, then check strip around dividing line.

Time complexity analysis uses the Master Theorem. For T(n) = a T(n/b) + f(n):

If f(n) < n^{log_b a}, T(n) = Θ(n^{log_b a}).
Equal, Θ(n^{log_b a} log n).
Greater, Θ(f(n)).
For Merge Sort: 2 T(n/2) + n → Θ(n log n).

Divide and Conquer is great for parallel processing since subproblems are independent.
In summary, Divide and Conquer simplifies complex problems through recursion. Practice with these algorithms, 
and you'll see the pattern. Next, we'll cover Dynamic Programming. Thanks for watching!"
